# ── Plugin Deployment ────────────────────────────────────
# Target vault for npm run deploy / npm run deploy:dev.
# Use the dedicated test vault to keep builds out of personal vaults.
OBSIDIAN_VAULT=/Users/you/obsidian-vaults/daily-digest-test

# ── Anthropic API ─────────────────────────────────────────
# Used by the plugin (as fallback when not set in Obsidian settings)
# and by AI evaluation tests.
ANTHROPIC_API_KEY=

# ── Local Model Server ───────────────────────────────────
# Only needed if you're running eval tests against a local model
# (Ollama, LM Studio, llama.cpp, vLLM, etc.)
#
# To get started with Ollama:
#   brew install ollama
#   ollama pull qwen3:8b
#   ollama serve
LOCAL_EVAL_ENDPOINT=http://localhost:11434
LOCAL_EVAL_MODEL=qwen3:8b

# ── AI Evaluation Tests ──────────────────────────────────
# Set to "true" to enable LLM-as-judge evaluation tests.
# These tests call a real AI model and cost money / take time.
DAILY_DIGEST_AI_EVAL=false

# Provider for eval tests: "anthropic" or "local"
#   anthropic — uses ANTHROPIC_API_KEY (best quality, costs money)
#   local     — uses LOCAL_EVAL_ENDPOINT + LOCAL_EVAL_MODEL (free, private)
DAILY_DIGEST_AI_EVAL_PROVIDER=local

# Model for the judge LLM
#   Anthropic default: claude-sonnet-4-20250514
#   Local default:     qwen3:8b (good JSON output, fast on 16GB+ RAM)
#   Other good local options: gemma3:12b, mistral-small, llama3.1:8b
DAILY_DIGEST_AI_EVAL_MODEL=qwen3:8b
